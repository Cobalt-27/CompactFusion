# ğŸ³CompactFusion: Accelerating Parallel Diffusion Model Serving with Residual Compression

> **TL;DR**: Diffusion models exhibit heavy temporal redundancy, yet we transmit full activations step after step.  
> **Why are we sending near-duplicated data across GPUs?**  
> CompactFusion compresses only the residuals â€” the real information change â€” to drastically reduce bandwidth with minimal quality loss.

![Teaser Image](https://img.picgo.net/2025/05/15/teaser741bf3f5ec634b23.png)

## â˜˜ï¸ Acknowledgements
We owe special thanks to the **xDiT** teamâ€”without their excellent open-source framework, this project would simply not exist.

Their work laid the foundation for everything we've built.

We thank the **DistriFusion** authors for sharing their code and system.

We also thank [common_metrics_on_video_quality](https://github.com/JunyaoHu/common_metrics_on_video_quality) for their excellent video quality evaluation tools.

## ğŸ€ Motivation

- Diffusion models generate data step-by-step, but their intermediate activations change **slowly and predictably**.
- In multi-GPU inference, these large activations are repeatedly transmitted between devices.
- The transmitted data are **highly redundant**, wasting precious bandwidth on near-duplicate content.
- We ask: **Why are we transmitting redundant stuff at all?**


## ğŸš€ Introducing CompactFusion

CompactFusion is a residual compression framework for parallel diffusion model serving.  
It compresses only the **change** (residual) between activations across steps â€” and adds optional **error feedback** to maintain reconstruction quality.

âœ… CompactFusion only targets **communication**, making it:
- ğŸ“¦ **Plug-and-play**: No model re-training or modification
- ğŸ›  **Framework-compatible**: Integrates into ring attention, patch parallelism and more
- âš¡ **Extremely efficient**: Up to **100Ã— compression**, with **<1% data transmitted**, but still **outperforming DistriFusion in quality**.

![Intro Image](https://img.picgo.net/2025/05/15/intro40850a8451398a26.png)

## ğŸš Method Illustration

<table>
<tr>
<td><b>Residual Compression Principle</b><br><img src="https://img.picgo.net/2025/05/15/residualae697c4a98859629.png" alt="Residual Illustration"></td>
<td><b>System Architecture</b><br><img src="https://img.picgo.net/2025/05/15/archce65b198fc390fc9.png" alt="System Diagram"></td>
</tr>
</table>

## âœ¨ Supported Features

CompactFusion supports out-of-the-box compression for:

- âœ… Compressed Ring Attention
- âœ… Compressed Patch Parallel
- âœ… DistriFusion migrated to `xDiT`
- âœ… Patch Parallel migrated to `xDiT`
- âœ… FLUX, CogVideoX, SD, Pixart-alpha and other backbones


## ğŸ’¾ Installation & Setup

We build CompactFusion on top of the excellent [`xDiT`](https://github.com/xdit-project/xDiT) framework.

## ğŸ³ Recommended Setup

You may simply use the pre-built Docker image from xDiT:

```bash
docker pull thufeifeibear/xdit-dev
```

## ğŸ”§ Code Examples
Example usages are provided in:

`examples/cogvideox_example.py`

`examples/flux_example.py`

**We do not modify the setup of xDiT. You can refer directly to xDiT documentation for usage details.**
