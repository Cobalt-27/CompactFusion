GPU Model,Release Date,Peak FP32 (TFLOPS),Peak FP16 (TFLOPS),Memory Bandwidth (GB/s)
NVIDIA A100 PCIe 40 GB,2020-05,19.5,312,1555
NVIDIA A100 SXM4 40 GB,2020-05,19.5,312,1555
AMD Instinct MI100,2020-11,23.1,184.6,1228.8
NVIDIA A100 SXM4 80 GB,2020-11,19.5,624,2039
NVIDIA A100 PCIe 80 GB,2021-06,19.5,312,1935
AMD Instinct MI250,2021-11,45.3,362.1,3276.8
AMD Instinct MI250X,2021-11,47.9,383,3200
AMD Instinct MI210,2022-03,22.6,181,1638.4
NVIDIA H100 PCIe 80 GB,2023-03,51,1513,2000
NVIDIA H100 SXM5 80 GB,2023-03,67,1979,3350
AMD Instinct MI300A,2023-12,122.6,1961.2,5300
AMD Instinct MI300X,2023-12,163.4,2614.9,5300
AMD Instinct MI325X,2024-10,163.4,2614.9,6000